{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy\n",
    "import sklearn \n",
    "#Base Naive Bayes classifier:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from evaluation import compute_macro_f1_score\n",
    "from evaluation import compute_micro_f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800003\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('data/train.json', 'r', encoding='utf-8') as fp:\n",
    "        train_data = json.load(fp)\n",
    "print(len(train_data ))\n",
    "\n",
    "with open('data/valid.json', 'r', encoding='utf-8') as fp:\n",
    "        valid_old = json.load(fp)\n",
    "\n",
    "with open('data/valid_new.json', 'r', encoding='utf-8') as fp:\n",
    "        valid_new = json.load(fp)\n",
    "def get_text_langid(data):\n",
    "        X = ['^' + i['text'] + '$' for i in data]\n",
    "        y = [i['langid'] for i in data]\n",
    "        return X, y\n",
    "X_valid_old, y_valid_old = get_text_langid(valid_old)\n",
    "X_valid_new, y_valid_new = get_text_langid(valid_new)\n",
    "X_train, y_train = get_text_langid(train_data)\n",
    "# X_valid_old_cnt = vec.transform(X_valid_old)\n",
    "# X_valid_new_cnt = vec.transform(X_valid_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the predicted variable (language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = set();\n",
    "for i in train_data:\n",
    "    languages.add(i['langid'])\n",
    "\n",
    "languages = list(languages); #convert it into a list.\n",
    "langind = {};\n",
    "for i in languages:\n",
    "    langind[i] = len(langind)\n",
    "langname = {};\n",
    "for i in langind:\n",
    "    langname[langind[i]] = i;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Simple NaiveBayes classifier at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_space(text): #A global function for consistent tokenization.\n",
    "    return text[1:-1].lower().split(); #simple for now.\n",
    "\n",
    "def split_by_punctuation(text):\n",
    "    t = re.split(' |;|\\*|\\n|:|@|#|!|\\(|\\)', text[1:-1]);\n",
    "    if(t[-1] == ''):\n",
    "        t = t[:-1];\n",
    "    return t;\n",
    "\n",
    "#So we cannot do the bigram testing SINCE bigrams would be too big to handle. but we can maybe try tho\n",
    "\n",
    "def unigram(text):\n",
    "    return [x for x in text];\n",
    "def quadgram(text):\n",
    "    s = text.lower();\n",
    "    return [text[i:i+4].lower() for i in range(len(text)-3)];\n",
    "def pentagram(text):\n",
    "    s = text.lower();\n",
    "    return [s[i:i+5] for i in range(len(s)-4)];\n",
    "def word_and_quadgram(text):\n",
    "    s = text.lower();\n",
    "    return split_by_space(s) + quadgram(s);\n",
    "def word_and_pentagram(text):\n",
    "    s = text.lower();\n",
    "    return split_by_punctuation(s) + pentagram(s);\n",
    "pentauniparameter = 1;\n",
    "def penta_and_unigram(text):\n",
    "    s = text.lower();\n",
    "    return pentauniparameter*pentagram(s) + unigram(s);\n",
    "\n",
    "def hexagram(text):\n",
    "    s = text.lower();\n",
    "    return [s[i:i+6] for i in range(len(s) - 5)];\n",
    "\n",
    "def hexa_penta(text):\n",
    "    return hexagram(text) + pentagram(text);\n",
    "\n",
    "\n",
    "# def get_vocab(data):\n",
    "#     vocab = set();\n",
    "#     for i in range(len(data)):\n",
    "#         strlist = unigram(data[i]['text']); #The string we need to determine the language of.\n",
    "#         for j in strlist:\n",
    "#             # if(j == 'à¤¤'):\n",
    "#             #     print('found here: ', data[i]);\n",
    "#             vocab.add(j); #we add all the words to the vocabulary.\n",
    "#     return vocab\n",
    "# vocab = get_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get each word in the vocabulary and assign it a unique index.\n",
    "wordind = {};\n",
    "for i in vocab:\n",
    "    wordind[i] = len(wordind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then we create a matrix of the size of the vocabulary and the number of languages.\n",
    "## This denotes the frequencies of each word in the vocabulary for each of the languages.\n",
    "def get_freq_matrix(data, wordind, langind):\n",
    "    matrix = numpy.zeros((len(wordind), len(langind)));\n",
    "    for i in range(len(data)):\n",
    "        strlist = unigram(data[i]['text'])\n",
    "        for j in strlist:\n",
    "            matrix[wordind[j]][langind[data[i]['langid']]] += 1;\n",
    "    return matrix\n",
    "train_matrix = get_freq_matrix(train_data, wordind, langind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set language counts:\n",
      "total= 800003\n",
      "en = 39.47510196836762,fr = 10.297086385926052,de = 10.120087049673563,es = 9.813713198575506,it = 9.31634006372476,pt = 9.194090522160542,sv = 9.09934087747171,kn = 0.47349822438165856,bn = 0.4502483115688316,hi = 0.44599832750627183,ml = 0.44512333078750954,ta = 0.44462333266250254,mr = 0.424748407193473,\n",
      "Validation set language counts:\n",
      "Old validation set:\n",
      "total= 100000\n",
      "en = 39.475,fr = 10.297,de = 10.12,es = 9.814,it = 9.316,pt = 9.194,sv = 9.099,kn = 0.473,bn = 0.451,hi = 0.446,ta = 0.445,ml = 0.445,mr = 0.425,\n",
      "New validation set:\n",
      "total= 2446\n",
      "en = 33.19705641864268,fr = 8.667211774325429,de = 8.503679476696648,es = 8.258381030253474,it = 7.849550286181521,pt = 7.726901062959935,sv = 7.686017988552739,mr = 4.170073589533933,bn = 3.0253475061324613,hi = 2.9026982829108747,ta = 2.7800490596892886,ml = 2.7800490596892886,kn = 2.4529844644317254,"
     ]
    }
   ],
   "source": [
    "def get_lang_cnts(data):\n",
    "    langcnt = {};\n",
    "    for i in data:\n",
    "        if i['langid'] in langcnt:\n",
    "            langcnt[i['langid']] += 1;\n",
    "        else:\n",
    "            langcnt[i['langid']] = 1;\n",
    "    return langcnt\n",
    "langcnt_train = get_lang_cnts(train_data)\n",
    "langcnt_valid_new = get_lang_cnts(valid_new)\n",
    "langcnt_valid_old = get_lang_cnts(valid_old)\n",
    "def print_sorted_vals(d):\n",
    "    total = 0;\n",
    "    for i in d:\n",
    "        total += d[i]\n",
    "    print(\"total=\", total)\n",
    "    for i in sorted(d, key=d.get, reverse=True):\n",
    "        print(i, \"=\", 100*d[i]/total, end = \",\")\n",
    "print(\"Training set language counts:\")\n",
    "print_sorted_vals(langcnt_train)\n",
    "print(\"\\nValidation set language counts:\")\n",
    "print(\"Old validation set:\")\n",
    "print_sorted_vals(langcnt_valid_old)\n",
    "print(\"\\nNew validation set:\")\n",
    "print_sorted_vals(langcnt_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_hexa_penta = CountVectorizer(tokenizer=hexa_penta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vec_hexa_penta.fit(X_train);\n",
    "X_train_vec = vec_hexa_penta.transform(X_train);\n",
    "X_valid_old_vec = vec_hexa_penta.transform(X_valid_old);\n",
    "X_valid_new_vec = vec_hexa_penta.transform(X_valid_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_valid_old, X_valid_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_smoothing = 0.01;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(y_to_train = y_train, x_to_train = X_train_vec, nb = None):\n",
    "    partial = True;\n",
    "    if(nb == None):\n",
    "        nb = MultinomialNB(alpha=laplace_smoothing)\n",
    "        partial = False;\n",
    "    if(partial):\n",
    "        nb.partial_fit(x_to_train, y_to_train);\n",
    "    else:\n",
    "        nb.fit(x_to_train, y_to_train);\n",
    "    return nb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(clf):\n",
    "    y_pred_old_valid = clf.predict(X_valid_old_vec)\n",
    "    y_pred_new_valid = clf.predict(X_valid_new_vec)\n",
    "    y_pred_train = clf.predict(X_train_vec)\n",
    "    print(\"accuracies: \", accuracy_score(y_valid_old, y_pred_old_valid), accuracy_score(y_valid_new, y_pred_new_valid), accuracy_score(y_train, y_pred_train))\n",
    "    #print(\"confusion matrix: \", confusion_matrix(y_valid_old, y_pred_old_valid), confusion_matrix(y_valid_new, y_pred_new_valid), confusion_matrix(y_train, y_pred_train))\n",
    "    print(\"micro F1  : \", compute_micro_f1_score(y_valid_old, y_pred_old_valid), compute_micro_f1_score(y_valid_new, y_pred_new_valid), compute_micro_f1_score(y_train, y_pred_train))\n",
    "    print(\"macro F1  : \", compute_macro_f1_score(y_valid_old, y_pred_old_valid), compute_macro_f1_score(y_valid_new, y_pred_new_valid), compute_macro_f1_score(y_train, y_pred_train))\n",
    "    print(\"-------------------------------------------------\\n\");\n",
    "    return y_pred_train; #for soft computation.\n",
    "    # return accuracy_score(y_valid_new, y_pred_new_valid);\n",
    "    # print(\"Conf matrix for valid_new\");\n",
    "    # conf = confusion_matrix(y_valid_new, y_pred_new_valid, labels=languages);\n",
    "    # disp = ConfusionMatrixDisplay(conf, display_labels=languages);\n",
    "    # disp.plot();\n",
    "    # plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedir/vect.pickle', 'rb') as fp:\n",
    "    vec = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X_train_vec = vec.transform(X_train)\n",
    "X_valid_old_vec = vec.transform(X_valid_old)\n",
    "X_valid_new_vec = vec.transform(X_valid_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedir/model.pickle', 'rb') as fp:\n",
    "    nb = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  0.99484 0.9856909239574816 0.9944712707327348\n",
      "micro F1  :  0.99150589319813 0.9787621359223301 0.9909015545320462\n",
      "macro F1  :  0.9960655398541703 0.9880808975421688 0.9957983516745131\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests(nb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  0.98663 0.9403107113654947 0.9945725203530487\n",
      "micro F1  :  0.9780938180984058 0.9137625516834021 0.9910681592838071\n",
      "macro F1  :  0.9769511078947956 0.9316449527740301 0.9943409648481386\n",
      "-------------------------------------------------\n",
      "\n",
      "new Best laplace smoothing:  0.02\n",
      "new Best accuracy:  0.9403107113654947\n",
      "accuracies:  0.98609 0.9370400654129191 0.9928087769670864\n",
      "micro F1  :  0.977226588081205 0.909358446144791 0.9881912956732891\n",
      "macro F1  :  0.9707740430347457 0.9272733044602783 0.9849475069566601\n",
      "-------------------------------------------------\n",
      "\n",
      "new Best laplace smoothing:  0.06\n",
      "new Best accuracy:  0.9403107113654947\n",
      "accuracies:  0.98577 0.9370400654129191 0.9918137806983224\n",
      "micro F1  :  0.9767118355590468 0.9094117647058824 0.9865725439380644\n",
      "macro F1  :  0.9685186546981891 0.9287010577293433 0.9800702287092946\n",
      "-------------------------------------------------\n",
      "\n",
      "new Best laplace smoothing:  0.1\n",
      "new Best accuracy:  0.9403107113654947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m laplace_smoothing \u001b[38;5;241m=\u001b[39m lap\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m50\u001b[39m;\n\u001b[0;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m train();\n\u001b[1;32m----> 6\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(acc \u001b[38;5;241m>\u001b[39m highest_acc):\n\u001b[0;32m      8\u001b[0m     highest_acc \u001b[38;5;241m=\u001b[39m acc;\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mrun_tests\u001b[1;34m(clf)\u001b[0m\n\u001b[0;32m      2\u001b[0m y_pred_old_valid \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_valid_old_vec)\n\u001b[0;32m      3\u001b[0m y_pred_new_valid \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_valid_new_vec)\n\u001b[1;32m----> 4\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracies: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_valid_old, y_pred_old_valid), accuracy_score(y_valid_new, y_pred_new_valid), accuracy_score(y_train, y_pred_train))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#print(\"confusion matrix: \", confusion_matrix(y_valid_old, y_pred_old_valid), confusion_matrix(y_valid_new, y_pred_new_valid), confusion_matrix(y_train, y_pred_train))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:102\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 102\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_joint_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:896\u001b[0m, in \u001b[0;36mMultinomialNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_joint_log_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the posterior log probability of the samples X\"\"\"\u001b[39;00m\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_log_prob_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_log_prior_\n",
      "File \u001b[1;32mc:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mc:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:678\u001b[0m, in \u001b[0;36m_spbase.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    677\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:580\u001b[0m, in \u001b[0;36m_spbase._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_vector(other\u001b[38;5;241m.\u001b[39mravel())\u001b[38;5;241m.\u001b[39mreshape(M, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m N:\n\u001b[1;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[1;32mc:\\Users\\tripl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:507\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    506\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matvecs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m   \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "highest_acc = 0;\n",
    "best_lap = 0.1;\n",
    "for lap in range(1, 100, 2):\n",
    "    laplace_smoothing = lap/100;\n",
    "    clf = train();\n",
    "    acc = run_tests(clf);\n",
    "    if(acc > highest_acc):\n",
    "        highest_acc = acc;\n",
    "        best_clf = clf;\n",
    "        best_lap = laplace_smoothing;\n",
    "    print(\"new Best laplace smoothing: \", laplace_smoothing);    \n",
    "    print(\"new Best accuracy: \", highest_acc);\n",
    "print(\"Highest accuracy: \", highest_acc);\n",
    "print(\"Best laplace smoothing: \", laplace_smoothing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_soft = [y_train]; x_soft = [X_train_vec];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = train(y_to_train=y_soft[-1], x_to_train = x_soft[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195589.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.max(nb.feature_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  0.98577 0.9370400654129191 0.9918137806983224\n",
      "micro F1  :  0.9767118355590468 0.9094117647058824 0.9865725439380644\n",
      "macro F1  :  0.9685186546981891 0.9287010577293433 0.9800702287092947\n",
      "-------------------------------------------------\n",
      "\n",
      "0 done\n",
      "accuracies:  0.98559 0.937857726901063 0.9912800326998774\n",
      "micro F1  :  0.9764226578094833 0.9104301708898055 0.9857053281258324\n",
      "macro F1  :  0.9643847981227918 0.9284620598655394 0.9772346989764169\n",
      "-------------------------------------------------\n",
      "\n",
      "1 done\n",
      "accuracies:  0.98552 0.9407195421095667 0.9911537831733132\n",
      "micro F1  :  0.9763085129010619 0.9143026004728133 0.9855004763514552\n",
      "macro F1  :  0.964174724221135 0.9316419673368143 0.9764025379531168\n",
      "-------------------------------------------------\n",
      "\n",
      "2 done\n",
      "accuracies:  0.9856 0.9415372035977105 0.9910975333842498\n",
      "micro F1  :  0.9764355495917131 0.9154346540508574 0.9854093853585784\n",
      "macro F1  :  0.965021841452408 0.9331096934721944 0.976034952926068\n",
      "-------------------------------------------------\n",
      "\n",
      "3 done\n",
      "accuracies:  0.98568 0.9411283728536386 0.9910662835014369\n",
      "micro F1  :  0.9765633950344511 0.914843287995269 0.9853586785731113\n",
      "macro F1  :  0.9660103862514112 0.9329436526108817 0.9758397273254195\n",
      "-------------------------------------------------\n",
      "\n",
      "4 done\n",
      "accuracies:  0.98577 0.9415372035977105 0.9910562835389367\n",
      "micro F1  :  0.9767080237011818 0.9153846153846154 0.9853424099085719\n",
      "macro F1  :  0.9668645960197763 0.9331599330624564 0.9757891438403324\n",
      "-------------------------------------------------\n",
      "\n",
      "5 done\n",
      "accuracies:  0.98577 0.9419460343417825 0.9910537835483116\n",
      "micro F1  :  0.9767068799004763 0.9159265837773831 0.9853383127485184\n",
      "macro F1  :  0.9680368559908802 0.9336811081483049 0.9757788300772194\n",
      "-------------------------------------------------\n",
      "\n",
      "6 done\n",
      "accuracies:  0.98582 0.9415372035977105 0.9910512835576867\n",
      "micro F1  :  0.9767868251317814 0.9153345174659562 0.9853342456324363\n",
      "macro F1  :  0.9689426558773078 0.9327035965902682 0.9757690920213685\n",
      "-------------------------------------------------\n",
      "\n",
      "7 done\n",
      "accuracies:  0.98581 0.9415372035977105 0.9910450335811241\n",
      "micro F1  :  0.9767700744863714 0.9153345174659562 0.9853241230118734\n",
      "macro F1  :  0.9693106781283672 0.9327035965902682 0.9757317394934578\n",
      "-------------------------------------------------\n",
      "\n",
      "8 done\n",
      "accuracies:  0.98581 0.9431725265739984 0.9910362836139365\n",
      "micro F1  :  0.976769694191605 0.9175563463819691 0.9853099937519845\n",
      "macro F1  :  0.9699222430614908 0.9341639881499694 0.975667382325407\n",
      "-------------------------------------------------\n",
      "\n",
      "9 done\n",
      "accuracies:  0.98582 0.9435813573180704 0.9910312836326863\n",
      "micro F1  :  0.9767856850514873 0.9181008902077151 0.9853019200711244\n",
      "macro F1  :  0.9704399307988436 0.9348571738733101 0.975630658294031\n",
      "-------------------------------------------------\n",
      "\n",
      "10 done\n",
      "accuracies:  0.98579 0.9439901880621423 0.9910300336373739\n",
      "micro F1  :  0.9767365715501858 0.9185977421271538 0.9852998715582423\n",
      "macro F1  :  0.9707169213797813 0.9340859195029728 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "11 done\n",
      "accuracies:  0.98577 0.9443990188062142 0.9910300336373739\n",
      "micro F1  :  0.9767030664199997 0.9191438763376931 0.9852998715582423\n",
      "macro F1  :  0.9710816650653861 0.9340897905474681 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "12 done\n",
      "accuracies:  0.98579 0.9452166802943581 0.9910300336373739\n",
      "micro F1  :  0.976735428945645 0.9202855443188578 0.9852998715582423\n",
      "macro F1  :  0.971336934461469 0.9359771278630027 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "13 done\n",
      "accuracies:  0.98581 0.9443990188062142 0.9910300336373739\n",
      "micro F1  :  0.9767674121614984 0.9190957763236169 0.9852998715582423\n",
      "macro F1  :  0.9715664945437584 0.9347908030211652 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "14 done\n",
      "accuracies:  0.98578 0.9443990188062142 0.9910300336373739\n",
      "micro F1  :  0.976718675813291 0.9190957763236169 0.9852998715582423\n",
      "macro F1  :  0.9717073922072776 0.9349831083275512 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "15 done\n",
      "accuracies:  0.98576 0.9443990188062142 0.9910300336373739\n",
      "micro F1  :  0.9766859313348286 0.9190957763236169 0.9852998715582423\n",
      "macro F1  :  0.9719349744139092 0.9349831083275512 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "16 done\n",
      "accuracies:  0.9858 0.9443990188062142 0.9910300336373739\n",
      "micro F1  :  0.9767506590042078 0.9190957763236169 0.9852998715582423\n",
      "macro F1  :  0.9723712340808034 0.9349831083275512 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "17 done\n",
      "accuracies:  0.98581 0.9443990188062142 0.9910300336373739\n",
      "micro F1  :  0.976765890558994 0.9190957763236169 0.9852998715582423\n",
      "macro F1  :  0.9723520694244123 0.9353637014338055 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "18 done\n",
      "accuracies:  0.9858 0.9452166802943581 0.9910300336373739\n",
      "micro F1  :  0.9767483748423965 0.9202380952380951 0.9852998715582423\n",
      "macro F1  :  0.9724139666968982 0.9370142168005815 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "19 done\n",
      "accuracies:  0.98581 0.9456255110384301 0.9910300336373739\n",
      "micro F1  :  0.9767643687571639 0.9208333333333333 0.9852998715582423\n",
      "macro F1  :  0.972700311644349 0.9374316611076096 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "20 done\n",
      "accuracies:  0.98582 0.9452166802943581 0.9910300336373739\n",
      "micro F1  :  0.9767811236102242 0.9202380952380951 0.9852998715582423\n",
      "macro F1  :  0.972859851634862 0.936833330136109 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n",
      "21 done\n",
      "accuracies:  0.9858 0.9456255110384301 0.9910300336373739\n",
      "micro F1  :  0.9767487555671994 0.9208333333333333 0.9852998715582423\n",
      "macro F1  :  0.9729880210457346 0.9376451884989224 0.9756301328759717\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    y_new = run_tests(nb); \n",
    "    y_soft.append(y_new);\n",
    "    nb.feature_count_ = 1.1*nb.feature_count_;\n",
    "    nb = train(y_to_train=y_soft[-1], x_to_train=x_soft[-1], nb=nb);\n",
    "    print(i, \"done\");    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "with open('sept_11_model.pkl', 'wb') as f:\n",
    "    pickle.dump(nb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 6924248)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
